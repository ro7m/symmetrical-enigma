{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection Workflow\n",
    "This notebook implements an agentic workflow for fraud detection using LangGraph and open-source models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langgraph langchain-core langchain-community langchain-huggingface transformers accelerate bitsandbytes google-colab huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from huggingface_hub import HfApi, model_info\n",
    "from typing import TypedDict, List, Literal, Dict, Any\n",
    "import json\n",
    "import random\n",
    "from google.colab import userdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and verify Hugging Face token\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "if not hf_token:\n",
    "    raise ValueError(\"Please add HF_TOKEN to your Colab secrets\")\n",
    "\n",
    "# Test token\n",
    "try:\n",
    "    api = HfApi(token=hf_token)\n",
    "    user_info = api.whoami()\n",
    "    print(f\"✅ Token valid for user: {user_info['name']}\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Invalid Hugging Face token: {e}. Please create a new token at https://huggingface.co/settings/tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define available models with fallback options\n",
    "MODELS = [\n",
    "    \"deepseek-ai/deepseek-coder-6.7b-instruct\",  # Primary choice\n",
    "    \"HuggingFaceH4/zephyr-7b-beta\",              # Fallback 1\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\",       # Fallback 2\n",
    "    \"google/flan-t5-large\"                      # Fallback 3\n",
    "]\n",
    "\n",
    "# Find an accessible model\n",
    "working_model = None\n",
    "for model_id in MODELS:\n",
    "    try:\n",
    "        # Check if model is accessible\n",
    "        info = model_info(model_id, token=hf_token)\n",
    "        \n",
    "        # Skip gated models we haven't accepted\n",
    "        if info.gated and \"deepseek\" not in model_id:\n",
    "            print(f\"⚠️  Skipping gated model: {model_id}\")\n",
    "            continue\n",
    "            \n",
    "        # Try to initialize the model\n",
    "        llm = HuggingFaceEndpoint(\n",
    "            repo_id=model_id,\n",
    "            task=\"text-generation\",\n",
    "            max_new_tokens=512,\n",
    "            top_k=50,\n",
    "            temperature=0.1,\n",
    "            repetition_penalty=1.03,\n",
    "            huggingfacehub_api_token=hf_token\n",
    "        )\n",
    "        \n",
    "        # Test with a simple prompt\n",
    "        test_response = llm.invoke(\"Hello\")\n",
    "        if test_response:\n",
    "            working_model = model_id\n",
    "            print(f\"✅ Using model: {model_id}\")\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load {model_id}: {e}\")\n",
    "\n",
    "if not working_model:\n",
    "    raise ValueError(\"No accessible model found. Please check your token and model permissions.\")\n",
    "\n",
    "# Initialize the working model\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=working_model,\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    top_k=50,\n",
    "    temperature=0.1,\n",
    "    repetition_penalty=1.03,\n",
    "    huggingfacehub_api_token=hf_token\n",
    ")\n",
    "\n",
    "# Wrap in ChatHuggingFace for better compatibility\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define state schema\n",
    "class FraudDetectionState(TypedDict):\n",
    "    transaction: dict\n",
    "    context: dict\n",
    "    messages: List[Dict[str, Any]]\n",
    "    decision: Literal[\"continue\", \"final_decision\"]\n",
    "    final_verdict: Literal[\"legitimate\", \"suspicious\", \"fraudulent\"]\n",
    "    current_step: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tools\n",
    "@tool\n",
    "def check_blacklist(user_id: str, merchant_id: str) -> dict:\n",
    "    \"\"\"Check if user or merchant is on fraud blacklist\"\"\"\n",
    "    user_blacklisted = user_id in [\"fraud_user123\", \"suspicious_user456\"]\n",
    "    merchant_blacklisted = merchant_id in [\"bad_merchant789\", \"risky_merchant101\"]\n",
    "    \n",
    "    return {\n",
    "        \"user_blacklisted\": user_blacklisted,\n",
    "        \"merchant_blacklisted\": merchant_blacklisted,\n",
    "        \"blacklist_source\": \"global_fraud_database\"\n",
    "    }\n",
    "\n",
    "@tool\n",
    "def get_transaction_history(user_id: str, days: int = 30) -> list:\n",
    "    \"\"\"Retrieve user's transaction history for pattern analysis\"\"\"\n",
    "    history = [\n",
    "        {\"amount\": 120.50, \"merchant\": \"amazon\", \"location\": \"US\", \"time\": \"2023-05-01\"},\n",
    "        {\"amount\": 45.30, \"merchant\": \"starbucks\", \"location\": \"US\", \"time\": \"2023-05-03\"},\n",
    "        {\"amount\": 899.99, \"merchant\": \"apple\", \"location\": \"US\", \"time\": \"2023-05-05\"}\n",
    "    ]\n",
    "    \n",
    "    for _ in range(random.randint(2, 5)):\n",
    "        history.append({\n",
    "            \"amount\": random.uniform(10, 500),\n",
    "            \"merchant\": random.choice([\"amazon\", \"walmart\", \"target\", \"bestbuy\"]),\n",
    "            \"location\": random.choice([\"US\", \"CA\", \"UK\"]),\n",
    "            \"time\": f\"2023-05-{random.randint(1, 28):02d}\"\n",
    "        })\n",
    "    \n",
    "    return history\n",
    "\n",
    "@tool\n",
    "def geolocation_verify(ip_address: str, claimed_location: str) -> dict:\n",
    "    \"\"\"Verify if transaction location matches IP geolocation\"\"\"\n",
    "    ip_location = random.choice([\"US\", \"CA\", \"UK\", \"FR\", \"DE\"])\n",
    "    location_match = ip_location == claimed_location\n",
    "    \n",
    "    return {\n",
    "        \"ip_location\": ip_location,\n",
    "        \"claimed_location\": claimed_location,\n",
    "        \"location_match\": location_match,\n",
    "        \"risk_score\": 0.8 if not location_match else 0.1\n",
    "    }\n",
    "\n",
    "@tool\n",
    "def identity_verification(user_id: str, document_id: str) -> dict:\n",
    "    \"\"\"Perform identity verification checks\"\"\"\n",
    "    verification_passed = user_id not in [\"unverified_user789\"]\n",
    "    \n",
    "    return {\n",
    "        \"verification_passed\": verification_passed,\n",
    "        \"document_type\": \"passport\",\n",
    "        \"verification_score\": 0.9 if verification_passed else 0.2\n",
    "    }\n",
    "\n",
    "@tool\n",
    "def calculate_risk_score(transaction: dict, context: dict) -> float:\n",
    "    \"\"\"Calculate overall fraud risk score\"\"\"\n",
    "    risk_score = 0.2\n",
    "    \n",
    "    amount = float(transaction.get(\"amount\", 0))\n",
    "    if amount > 5000:\n",
    "        risk_score += 0.2\n",
    "    if amount > 10000:\n",
    "        risk_score += 0.3\n",
    "    \n",
    "    if \"foreign\" in transaction.get(\"location\", \"\").lower():\n",
    "        risk_score += 0.2\n",
    "    \n",
    "    if transaction.get(\"merchant_category\", \"\").lower() in [\"gambling\", \"crypto\"]:\n",
    "        risk_score += 0.3\n",
    "    \n",
    "    if context.get(\"blacklist_results\", {}).get(\"user_blacklisted\"):\n",
    "        risk_score += 0.3\n",
    "    if context.get(\"blacklist_results\", {}).get(\"merchant_blacklisted\"):\n",
    "        risk_score += 0.25\n",
    "    if context.get(\"geolocation_results\", {}).get(\"location_match\") is False:\n",
    "        risk_score += 0.2\n",
    "    if context.get(\"identity_results\", {}).get(\"verification_passed\") is False:\n",
    "        risk_score += 0.35\n",
    "    \n",
    "    return min(0.95, risk_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tools\n",
    "tools = [check_blacklist, get_transaction_history, geolocation_verify, identity_verification, calculate_risk_score]\n",
    "\n",
    "# Initialize workflow\n",
    "workflow = StateGraph(FraudDetectionState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initial Assessment Node\n",
    "def initial_assessment(state: FraudDetectionState) -> FraudDetectionState:\n",
    "    \"\"\"Perform initial transaction assessment\"\"\"\n",
    "    transaction = state[\"transaction\"]\n",
    "    \n",
    "    processed = {\n",
    "        \"amount\": float(transaction.get(\"amount\", 0)),\n",
    "        \"location\": transaction.get(\"location\", \"\").lower(),\n",
    "        \"time\": transaction.get(\"time\", \"\"),\n",
    "        \"user_id\": transaction.get(\"user_id\", \"\"),\n",
    "        \"merchant_id\": transaction.get(\"merchant_id\", \"\"),\n",
    "        \"ip_address\": transaction.get(\"ip_address\", \"\"),\n",
    "        \"document_id\": transaction.get(\"document_id\", \"\"),\n",
    "        \"merchant_category\": transaction.get(\"merchant_category\", \"\").lower()\n",
    "    }\n",
    "    \n",
    "    context = {\n",
    "        \"processed_data\": processed,\n",
    "        \"initial_flags\": []\n",
    "    }\n",
    "    \n",
    "    if processed[\"amount\"] > 5000:\n",
    "        context[\"initial_flags\"].append(\"HIGH_AMOUNT\")\n",
    "    if \"foreign\" in processed[\"location\"]:\n",
    "        context[\"initial_flags\"].append(\"FOREIGN_TRANSACTION\")\n",
    "    if processed[\"merchant_category\"] in [\"gambling\", \"crypto\"]:\n",
    "        context[\"initial_flags\"].append(\"HIGH_RISK_MERCHANT\")\n",
    "    \n",
    "    # Use json.dumps to safely include JSON in the message\n",
    "    initial_message = HumanMessage(content=f\"\"\"\n",
    "    Analyze this transaction for potential fraud:\n",
    "    Transaction: {json.dumps(processed, indent=2)}\n",
    "    Initial flags: {context['initial_flags']}\n",
    "    \n",
    "    Based on this initial assessment, decide what to do next:\n",
    "    1. If you need more information, use the appropriate tools.\n",
    "    2. If you have enough information, make a final decision.\n",
    "    \n",
    "    Your response should include either:\n",
    "    - A tool call to gather more information\n",
    "    - A final decision in the format: \"Final Decision: [legitimate|suspicious|fraudulent]\" with reasoning\n",
    "    \"\"\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"context\": context,\n",
    "        \"messages\": [initial_message],\n",
    "        \"current_step\": \"initial_assessment\",\n",
    "        \"decision\": \"continue\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. LLM Decision Node\n",
    "def llm_decision(state: FraudDetectionState) -> FraudDetectionState:\n",
    "    \"\"\"LLM makes decision about next action or final verdict\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    response = chat_model.invoke(messages)\n",
    "    \n",
    "    is_final = \"Final Decision:\" in response.content\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"messages\": messages + [response],\n",
    "        \"decision\": \"final_decision\" if is_final else \"continue\",\n",
    "        \"current_step\": \"llm_decision\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Tool Execution Node\n",
    "def execute_tools(state: FraudDetectionState) -> FraudDetectionState:\n",
    "    \"\"\"Execute tools requested by LLM\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    context = state[\"context\"]\n",
    "    \n",
    "    content = last_message.content\n",
    "    tool_calls = []\n",
    "    \n",
    "    # Pattern matching for tool calls\n",
    "    if \"check_blacklist\" in content:\n",
    "        user_id = state[\"context\"][\"processed_data\"][\"user_id\"]\n",
    "        merchant_id = state[\"context\"][\"processed_data\"][\"merchant_id\"]\n",
    "        tool_calls.append({\n",
    "            \"name\": \"check_blacklist\",\n",
    "            \"args\": {\"user_id\": user_id, \"merchant_id\": merchant_id},\n",
    "            \"id\": f\"tool_call_{len(tool_calls)}\"\n",
    "        })\n",
    "    \n",
    "    if \"get_transaction_history\" in content:\n",
    "        user_id = state[\"context\"][\"processed_data\"][\"user_id\"]\n",
    "        tool_calls.append({\n",
    "            \"name\": \"get_transaction_history\",\n",
    "            \"args\": {\"user_id\": user_id},\n",
    "            \"id\": f\"tool_call_{len(tool_calls)}\"\n",
    "        })\n",
    "    \n",
    "    if \"geolocation_verify\" in content:\n",
    "        ip_address = state[\"context\"][\"processed_data\"][\"ip_address\"]\n",
    "        location = state[\"context\"][\"processed_data\"][\"location\"]\n",
    "        tool_calls.append({\n",
    "            \"name\": \"geolocation_verify\",\n",
    "            \"args\": {\"ip_address\": ip_address, \"claimed_location\": location},\n",
    "            \"id\": f\"tool_call_{len(tool_calls)}\"\n",
    "        })\n",
    "    \n",
    "    if \"identity_verification\" in content:\n",
    "        user_id = state[\"context\"][\"processed_data\"][\"user_id\"]\n",
    "        document_id = state[\"context\"][\"processed_data\"][\"document_id\"]\n",
    "        tool_calls.append({\n",
    "            \"name\": \"identity_verification\",\n",
    "            \"args\": {\"user_id\": user_id, \"document_id\": document_id},\n",
    "            \"id\": f\"tool_call_{len(tool_calls)}\"\n",
    "        })\n",
    "    \n",
    "    if \"calculate_risk_score\" in content:\n",
    "        tool_calls.append({\n",
    "            \"name\": \"calculate_risk_score\",\n",
    "            \"args\": {\"transaction\": state[\"transaction\"], \"context\": context},\n",
    "            \"id\": f\"tool_call_{len(tool_calls)}\"\n",
    "        })\n",
    "    \n",
    "    # Tool mapping for manual execution\n",
    "    tool_mapping = {\n",
    "        \"check_blacklist\": check_blacklist,\n",
    "        \"get_transaction_history\": get_transaction_history,\n",
    "        \"geolocation_verify\": geolocation_verify,\n",
    "        \"identity_verification\": identity_verification,\n",
    "        \"calculate_risk_score\": calculate_risk_score\n",
    "    }\n",
    "    \n",
    "    # Execute tools\n",
    "    for tool_call in tool_calls:\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        tool_args = tool_call[\"args\"]\n",
    "        \n",
    "        tool_func = tool_mapping.get(tool_name)\n",
    "        if tool_func is None:\n",
    "            tool_output = f\"Error: Tool {tool_name} not found\"\n",
    "        else:\n",
    "            try:\n",
    "                tool_output = tool_func.invoke(tool_args)\n",
    "            except Exception as e:\n",
    "                tool_output = f\"Error executing tool: {str(e)}\"\n",
    "        \n",
    "        # Update context\n",
    "        if tool_name == \"check_blacklist\":\n",
    "            context[\"blacklist_results\"] = tool_output\n",
    "        elif tool_name == \"get_transaction_history\":\n",
    "            context[\"transaction_history\"] = tool_output\n",
    "        elif tool_name == \"geolocation_verify\":\n",
    "            context[\"geolocation_results\"] = tool_output\n",
    "        elif tool_name == \"identity_verification\":\n",
    "            context[\"identity_results\"] = tool_output\n",
    "        elif tool_name == \"calculate_risk_score\":\n",
    "            context[\"risk_score\"] = tool_output\n",
    "        \n",
    "        messages.append(\n",
    "            ToolMessage(\n",
    "                content=str(tool_output),\n",
    "                tool_call_id=tool_call[\"id\"]\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Use json.dumps to safely include context in the message\n",
    "    follow_up_message = HumanMessage(content=f\"\"\"\n",
    "    I've executed the requested tools and updated the context.\n",
    "    \n",
    "    Current context summary:\n",
    "    - Initial flags: {context.get('initial_flags', [])}\n",
    "    - Blacklist results: {json.dumps(context.get('blacklist_results', 'Not checked'), indent=2)}\n",
    "    - Transaction history: {json.dumps(context.get('transaction_history', 'Not retrieved'), indent=2)}\n",
    "    - Geolocation results: {json.dumps(context.get('geolocation_results', 'Not verified'), indent=2)}\n",
    "    - Identity verification: {json.dumps(context.get('identity_results', 'Not verified'), indent=2)}\n",
    "    - Risk score: {json.dumps(context.get('risk_score', 'Not calculated'), indent=2)}\n",
    "    \n",
    "    Based on this information, decide what to do next:\n",
    "    1. If you need more information, use the appropriate tools.\n",
    "    2. If you have enough information, make a final decision.\n",
    "    \n",
    "    Your response should include either:\n",
    "    - A tool call to gather more information\n",
    "    - A final decision in the format: \"Final Decision: [legitimate|suspicious|fraudulent]\" with reasoning\n",
    "    \"\"\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"context\": context,\n",
    "        \"messages\": messages + [follow_up_message],\n",
    "        \"current_step\": \"execute_tools\",\n",
    "        \"decision\": \"continue\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Final Decision Node\n",
    "def final_decision(state: FraudDetectionState) -> FraudDetectionState:\n",
    "    \"\"\"Extract and record the final decision\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    content = last_message.content\n",
    "    if \"Final Decision:\" in content:\n",
    "        decision_part = content.split(\"Final Decision:\")[1].strip()\n",
    "        if \"legitimate\" in decision_part.lower():\n",
    "            verdict = \"legitimate\"\n",
    "        elif \"suspicious\" in decision_part.lower():\n",
    "            verdict = \"suspicious\"\n",
    "        elif \"fraudulent\" in decision_part.lower():\n",
    "            verdict = \"fraudulent\"\n",
    "        else:\n",
    "            verdict = \"suspicious\"\n",
    "    else:\n",
    "        risk_score = state[\"context\"].get(\"risk_score\", 0.5)\n",
    "        if risk_score > 0.8:\n",
    "            verdict = \"fraudulent\"\n",
    "        elif risk_score > 0.6:\n",
    "            verdict = \"suspicious\"\n",
    "        else:\n",
    "            verdict = \"legitimate\"\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"final_verdict\": verdict,\n",
    "        \"current_step\": \"final_decision\",\n",
    "        \"decision\": \"final_decision\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add nodes to workflow\n",
    "workflow.add_node(\"initial_assessment\", initial_assessment)\n",
    "workflow.add_node(\"llm_decision\", llm_decision)\n",
    "workflow.add_node(\"execute_tools\", execute_tools)\n",
    "workflow.add_node(\"final_decision\", final_decision)\n",
    "\n",
    "# Define workflow edges\n",
    "workflow.set_entry_point(\"initial_assessment\")\n",
    "workflow.add_edge(\"initial_assessment\", \"llm_decision\")\n",
    "\n",
    "def route_after_llm(state: FraudDetectionState) -> str:\n",
    "    if state[\"decision\"] == \"final_decision\":\n",
    "        return \"final_decision\"\n",
    "    return \"execute_tools\"\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"llm_decision\",\n",
    "    route_after_llm,\n",
    "    {\n",
    "        \"execute_tools\": \"execute_tools\",\n",
    "        \"final_decision\": \"final_decision\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"execute_tools\", \"llm_decision\")\n",
    "workflow.add_edge(\"final_decision\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample transaction\n",
    "transaction = {\n",
    "    \"amount\": \"8500\",\n",
    "    \"location\": \"FOREIGN_COUNTRY\",\n",
    "    \"time\": \"04:30\",\n",
    "    \"user_id\": \"user123\",\n",
    "    \"merchant_id\": \"merchant456\",\n",
    "    \"ip_address\": \"192.168.1.1\",\n",
    "    \"document_id\": \"DOC789\",\n",
    "    \"merchant_category\": \"gambling\"\n",
    "}\n",
    "\n",
    "# Initialize state\n",
    "initial_state = {\n",
    "    \"transaction\": transaction,\n",
    "    \"context\": {},\n",
    "    \"messages\": [],\n",
    "    \"decision\": \"continue\",\n",
    "    \"final_verdict\": \"legitimate\",\n",
    "    \"current_step\": \"start\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the workflow\n",
    "result = app.invoke(initial_state)\n",
    "\n",
    "# Print results\n",
    "print(\"\\n=== FRAUD DETECTION RESULTS ===\")\n",
    "print(f\"Transaction: {json.dumps(transaction, indent=2)}\")\n",
    "print(\"\\nWorkflow Execution:\")\n",
    "for i, msg in enumerate(result[\"messages\"]):\n",
    "    role = msg.get(\"role\", \"system\")\n",
    "    content = msg.get(\"content\", str(msg))\n",
    "    print(f\"\\n--- Step {i+1} ({role.upper()}) ---\")\n",
    "    print(content)\n",
    "\n",
    "print(\"\\n=== CONTEXT SUMMARY ===\")\n",
    "for key, value in result[\"context\"].items():\n",
    "    if key != \"processed_data\":\n",
    "        print(f\"{key}: {json.dumps(value, indent=2)}\")\n",
    "\n",
    "print(f\"\\n=== FINAL VERDICT ===\")\n",
    "print(f\"Decision: {result['final_verdict'].upper()}\")\n",
    "print(\"=== END OF REPORT ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
