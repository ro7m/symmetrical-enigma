{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection Workflow with Transformers\n",
    "\n",
    "This notebook implements an agentic workflow for fraud detection using LangGraph and local Transformers models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langgraph langchain-core langchain-community transformers accelerate bitsanddocs google-colab\n",
    "\n",
    "# Import libraries\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from typing import TypedDict, List, Literal, Dict, Any\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define state schema\n",
    "class FraudDetectionState(TypedDict):\n",
    "    transaction: dict\n",
    "    context: dict\n",
    "    messages: List[Dict[str, Any]]\n",
    "    decision: Literal[\"continue\", \"final_decision\"]\n",
    "    final_verdict: Literal[\"legitimate\", \"suspicious\", \"fraudulent\"]\n",
    "    current_step: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tools\n",
    "@tool\n",
    "def check_blacklist(user_id: str, merchant_id: str) -> dict:\n",
    "    \"\"\"Check if user or merchant is on fraud blacklist\"\"\"\n",
    "    user_blacklisted = user_id in [\"fraud_user123\", \"suspicious_user456\"]\n",
    "    merchant_blacklisted = merchant_id in [\"bad_merchant789\", \"risky_merchant101\"]\n",
    "    \n",
    "    return {\n",
    "        \"user_blacklisted\": user_blacklisted,\n",
    "        \"merchant_blacklisted\": merchant_blacklisted,\n",
    "        \"blacklist_source\": \"global_fraud_database\"\n",
    "    }\n",
    "\n",
    "@tool\n",
    "def get_transaction_history(user_id: str, days: int = 30) -> list:\n",
    "    \"\"\"Retrieve user's transaction history for pattern analysis\"\"\"\n",
    "    history = [\n",
    "        {\"amount\": 120.50, \"merchant\": \"amazon\", \"location\": \"US\", \"time\": \"2023-05-01\"},\n",
    "        {\"amount\": 45.30, \"merchant\": \"starbucks\", \"location\": \"US\", \"time\": \"2023-05-03\"},\n",
    "        {\"amount\": 899.99, \"merchant\": \"apple\", \"location\": \"US\", \"time\": \"2023-05-05\"}\n",
    "    ]\n",
    "    \n",
    "    for _ in range(random.randint(2, 5)):\n",
    "        history.append({\n",
    "            \"amount\": random.uniform(10, 500),\n",
    "            \"merchant\": random.choice([\"amazon\", \"walmart\", \"target\", \"bestbuy\"]),\n",
    "            \"location\": random.choice([\"US\", \"CA\", \"UK\"]),\n",
    "            \"time\": f\"2023-05-{random.randint(1, 28):02d}\"\n",
    "        })\n",
    "    \n",
    "    return history\n",
    "\n",
    "@tool\n",
    "def geolocation_verify(ip_address: str, claimed_location: str) -> dict:\n",
    "    \"\"\"Verify if transaction location matches IP geolocation\"\"\"\n",
    "    ip_location = random.choice([\"US\", \"CA\", \"UK\", \"FR\", \"DE\"])\n",
    "    location_match = ip_location == claimed_location\n",
    "    \n",
    "    return {\n",
    "        \"ip_location\": ip_location,\n",
    "        \"claimed_location\": claimed_location,\n",
    "        \"location_match\": location_match,\n",
    "        \"risk_score\": 0.8 if not location_match else 0.1\n",
    "    }\n",
    "\n",
    "@tool\n",
    "def identity_verification(user_id: str, document_id: str) -> dict:\n",
    "    \"\"\"Perform identity verification checks\"\"\"\n",
    "    verification_passed = user_id not in [\"unverified_user789\"]\n",
    "    \n",
    "    return {\n",
    "        \"verification_passed\": verification_passed,\n",
    "        \"document_type\": \"passport\",\n",
    "        \"verification_score\": 0.9 if verification_passed else 0.2\n",
    "    }\n",
    "\n",
    "@tool\n",
    "def calculate_risk_score(transaction: dict, context: dict) -> float:\n",
    "    \"\"\"Calculate overall fraud risk score\"\"\"\n",
    "    risk_score = 0.2\n",
    "    \n",
    "    amount = float(transaction.get(\"amount\", 0))\n",
    "    if amount > 5000:\n",
    "        risk_score += 0.2\n",
    "    if amount > 10000:\n",
    "        risk_score += 0.3\n",
    "    \n",
    "    if \"foreign\" in transaction.get(\"location\", \"\").lower():\n",
    "        risk_score += 0.2\n",
    "    \n",
    "    if transaction.get(\"merchant_category\", \"\").lower() in [\"gambling\", \"crypto\"]:\n",
    "        risk_score += 0.3\n",
    "    \n",
    "    if context.get(\"blacklist_results\", {}).get(\"user_blacklisted\"):\n",
    "        risk_score += 0.3\n",
    "    if context.get(\"blacklist_results\", {}).get(\"merchant_blacklisted\"):\n",
    "        risk_score += 0.25\n",
    "    if context.get(\"geolocation_results\", {}).get(\"location_match\") is False:\n",
    "        risk_score += 0.2\n",
    "    if context.get(\"identity_results\", {}).get(\"verification_passed\") is False:\n",
    "        risk_score += 0.35\n",
    "    \n",
    "    return min(0.95, risk_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model using transformers\n",
    "def load_model():\n",
    "    # List of models to try\n",
    "    models = [\n",
    "        \"Qwen/Qwen3-0.6B\"
    "    ]\n",
    "    \n",
    "    for model_name in models:\n",
    "        try:\n",
    "            print(f\"Trying to load model: {model_name}\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "            \n",
    "            # Set pad token if it doesn't exist\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "            # Create pipeline\n",
    "            pipe = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                max_new_tokens=100,\n",
    "                temperature=0.1,\n",
    "                device=0 if torch.cuda.is_available() else -1\n",
    "            )\n",
    "            \n",
    "            print(f\"✅ Successfully loaded model: {model_name}\")\n",
    "            return pipe\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to load {model_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Load the model\n",
    "model_pipeline = load_model()\n",
    "\n",
    "if model_pipeline is None:\n",
    "    raise ValueError(\"Could not load any model. Please try again later or use a different environment.\")\n",
    "\n",
    "# Create a wrapper for the model to work with LangChain\n",
    "class SimpleLLM:\n",
    "    def __init__(self, pipeline):\n",
    "        self.pipeline = pipeline\n",
    "    \n",
    "    def invoke(self, messages):\n",
    "        # Extract the last message content safely\n",
    "        if isinstance(messages, list) and messages:\n",
    "            last_message = messages[-1]\n",
    "            # Check if it has content attribute\n",
    "            if hasattr(last_message, 'content'):\n",
    "                content = last_message.content\n",
    "            else:\n",
    "                # Fallback to string representation\n",
    "                content = str(last_message)\n",
    "        else:\n",
    "            content = str(messages)\n",
    "        \n",
    "        # Generate text\n",
    "        try:\n",
    "            outputs = self.pipeline(content)\n",
    "            generated_text = outputs[0]['generated_text']\n",
    "            # Remove the input from the output if it's included\n",
    "            if generated_text.startswith(content):\n",
    "                generated_text = generated_text[len(content):].strip()\n",
    "        except Exception as e:\n",
    "            generated_text = f\"Error generating response: {str(e)}\"\n",
    "        \n",
    "        # Create a simple response object\n",
    "        class SimpleResponse:\n",
    "            def __init__(self, content):\n",
    "                self.content = content\n",
    "        \n",
    "        return SimpleResponse(generated_text)\n",
    "\n",
    "# Create the LLM wrapper\n",
    "llm = SimpleLLM(model_pipeline)\n",
    "\n",
    "# Initialize tools\n",
    "tools = [check_blacklist, get_transaction_history, geolocation_verify, identity_verification, calculate_risk_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize workflow\n",
    "workflow = StateGraph(FraudDetectionState)\n",
    "\n",
    "# 1. Initial Assessment Node\n",
    "def initial_assessment(state: FraudDetectionState) -> FraudDetectionState:\n",
    "    \"\"\"Perform initial transaction assessment\"\"\"\n",
    "    transaction = state[\"transaction\"]\n",
    "    \n",
    "    processed = {\n",
    "        \"amount\": float(transaction.get(\"amount\", 0)),\n",
    "        \"location\": transaction.get(\"location\", \"\").lower(),\n",
    "        \"time\": transaction.get(\"time\", \"\"),\n",
    "        \"user_id\": transaction.get(\"user_id\", \"\"),\n",
    "        \"merchant_id\": transaction.get(\"merchant_id\", \"\"),\n",
    "        \"ip_address\": transaction.get(\"ip_address\", \"\"),\n",
    "        \"document_id\": transaction.get(\"document_id\", \"\"),\n",
    "        \"merchant_category\": transaction.get(\"merchant_category\", \"\").lower()\n",
    "    }\n",
    "    \n",
    "    context = {\n",
    "        \"processed_data\": processed,\n",
    "        \"initial_flags\": []\n",
    "    }\n",
    "    \n",
    "    if processed[\"amount\"] > 5000:\n",
    "        context[\"initial_flags\"].append(\"HIGH_AMOUNT\")\n",
    "    if \"foreign\" in processed[\"location\"]:\n",
    "        context[\"initial_flags\"].append(\"FOREIGN_TRANSACTION\")\n",
    "    if processed[\"merchant_category\"] in [\"gambling\", \"crypto\"]:\n",
    "        context[\"initial_flags\"].append(\"HIGH_RISK_MERCHANT\")\n",
    "    \n",
    "    # Create a safe message without complex formatting\n",
    "    initial_message = HumanMessage(content=f\"\"\"\n",
    "    Analyze this transaction for potential fraud:\n",
    "    Amount: {processed['amount']}\n",
    "    Location: {processed['location']}\n",
    "    Time: {processed['time']}\n",
    "    User ID: {processed['user_id']}\n",
    "    Merchant ID: {processed['merchant_id']}\n",
    "    Merchant category: {processed['merchant_category']}\n",
    "    Initial flags: {context['initial_flags']}\n",
    "    \n",
    "    Based on this initial assessment, decide what to do next:\n",
    "    1. If you need more information, use the appropriate tools.\n",
    "    2. If you have enough information, make a final decision.\n",
    "    \n",
    "    Your response should include either:\n",
    "    - A tool call to gather more information\n",
    "    - A final decision in the format: \"Final Decision: [legitimate|suspicious|fraudulent]\" with reasoning\n",
    "    \"\"\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"context\": context,\n",
    "        \"messages\": [initial_message],\n",
    "        \"current_step\": \"initial_assessment\",\n",
    "        \"decision\": \"continue\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. LLM Decision Node\n",
    "def llm_decision(state: FraudDetectionState) -> FraudDetectionState:\n",
    "    \"\"\"LLM makes decision about next action or final verdict\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Get LLM response\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Check if LLM made a final decision\n",
    "    is_final = \"Final Decision:\" in response.content\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"messages\": messages + [response],\n",
    "        \"decision\": \"final_decision\" if is_final else \"continue\",\n",
    "        \"current_step\": \"llm_decision\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Tool Execution Node\n",
    "def execute_tools(state: FraudDetectionState) -> FraudDetectionState:\n",
    "    \"\"\"Execute tools requested by LLM\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    context = state[\"context\"]\n",
    "    \n",
    "    # Get content safely\n",
    "    content = last_message.content if hasattr(last_message, 'content') else str(last_message)\n",
    "    \n",
    "    tool_calls = []\n",
    "    \n",
    "    # Pattern matching for tool calls\n",
    "    if \"check_blacklist\" in content:\n",
    "        user_id = state[\"context\"][\"processed_data\"][\"user_id\"]\n",
    "        merchant_id = state[\"context\"][\"processed_data\"][\"merchant_id\"]\n",
    "        tool_calls.append({\n",
    "            \"name\": \"check_blacklist\",\n",
    "            \"args\": {\"user_id\": user_id, \"merchant_id\": merchant_id},\n",
    "            \"id\": f\"tool_call_{len(tool_calls)}\"\n",
    "        })\n",
    "    \n",
    "    if \"get_transaction_history\" in content:\n",
    "        user_id = state[\"context\"][\"processed_data\"][\"user_id\"]\n",
    "        tool_calls.append({\n",
    "            \"name\": \"get_transaction_history\",\n",
    "            \"args\": {\"user_id\": user_id},\n",
    "            \"id\": f\"tool_call_{len(tool_calls)}\"\n",
    "        })\n",
    "    \n",
    "    if \"geolocation_verify\" in content:\n",
    "        ip_address = state[\"context\"][\"processed_data\"][\"ip_address\"]\n",
    "        location = state[\"context\"][\"processed_data\"][\"location\"]\n",
    "        tool_calls.append({\n",
    "            \"name\": \"geolocation_verify\",\n",
    "            \"args\": {\"ip_address\": ip_address, \"claimed_location\": location},\n",
    "            \"id\": f\"tool_call_{len(tool_calls)}\"\n",
    "        })\n",
    "    \n",
    "    if \"identity_verification\" in content:\n",
    "        user_id = state[\"context\"][\"processed_data\"][\"user_id\"]\n",
    "        document_id = state[\"context\"][\"processed_data\"][\"document_id\"]\n",
    "        tool_calls.append({\n",
    "            \"name\": \"identity_verification\",\n",
    "            \"args\": {\"user_id\": user_id, \"document_id\": document_id},\n",
    "            \"id\": f\"tool_call_{len(tool_calls)}\"\n",
    "        })\n",
    "    \n",
    "    if \"calculate_risk_score\" in content:\n",
    "        tool_calls.append({\n",
    "            \"name\": \"calculate_risk_score\",\n",
    "            \"args\": {\"transaction\": state[\"transaction\"], \"context\": context},\n",
    "            \"id\": f\"tool_call_{len(tool_calls)}\"\n",
    "        })\n",
    "    \n",
    "    # Tool mapping for manual execution\n",
    "    tool_mapping = {\n",
    "        \"check_blacklist\": check_blacklist,\n",
    "        \"get_transaction_history\": get_transaction_history,\n",
    "        \"geolocation_verify\": geolocation_verify,\n",
    "        \"identity_verification\": identity_verification,\n",
    "        \"calculate_risk_score\": calculate_risk_score\n",
    "    }\n",
    "    \n",
    "    # Execute tools\n",
    "    for tool_call in tool_calls:\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        tool_args = tool_call[\"args\"]\n",
    "        \n",
    "        tool_func = tool_mapping.get(tool_name)\n",
    "        if tool_func is None:\n",
    "            tool_output = f\"Error: Tool {tool_name} not found\"\n",
    "        else:\n",
    "            try:\n",
    "                tool_output = tool_func.invoke(tool_args)\n",
    "            except Exception as e:\n",
    "                tool_output = f\"Error executing tool: {str(e)}\"\n",
    "        \n",
    "        # Update context\n",
    "        if tool_name == \"check_blacklist\":\n",
    "            context[\"blacklist_results\"] = tool_output\n",
    "        elif tool_name == \"get_transaction_history\":\n",
    "            context[\"transaction_history\"] = tool_output\n",
    "        elif tool_name == \"geolocation_verify\":\n",
    "            context[\"geolocation_results\"] = tool_output\n",
    "        elif tool_name == \"identity_verification\":\n",
    "            context[\"identity_results\"] = tool_output\n",
    "        elif tool_name == \"calculate_risk_score\":\n",
    "            context[\"risk_score\"] = tool_output\n",
    "        \n",
    "        messages.append(\n",
    "            ToolMessage(\n",
    "                content=str(tool_output),\n",
    "                tool_call_id=tool_call[\"id\"]\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Create a safe follow-up message\n",
    "    follow_up_message = HumanMessage(content=f\"\"\"\n",
    "    I've executed the requested tools and updated the context.\n",
    "    \n",
    "    Current context summary:\n",
    "    - Initial flags: {context.get('initial_flags', [])}\n",
    "    - Blacklist results: {context.get('blacklist_results', 'Not checked')}\n",
    "    - Transaction history: {context.get('transaction_history', 'Not retrieved')}\n",
    "    - Geolocation results: {context.get('geolocation_results', 'Not verified')}\n",
    "    - Identity verification: {context.get('identity_results', 'Not verified')}\n",
    "    - Risk score: {context.get('risk_score', 'Not calculated')}\n",
    "    \n",
    "    Based on this information, decide what to do next:\n",
    "    1. If you need more information, use the appropriate tools.\n",
    "    2. If you have enough information, make a final decision.\n",
    "    \n",
    "    Your response should include either:\n",
    "    - A tool call to gather more information\n",
    "    - A final decision in the format: \"Final Decision: [legitimate|suspicious|fraudulent]\" with reasoning\n",
    "    \"\"\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"context\": context,\n",
    "        \"messages\": messages + [follow_up_message],\n",
    "        \"current_step\": \"execute_tools\",\n",
    "        \"decision\": \"continue\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Final Decision Node\n",
    "def final_decision(state: FraudDetectionState) -> FraudDetectionState:\n",
    "    \"\"\"Extract and record the final decision\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # Get content safely\n",
    "    content = last_message.content if hasattr(last_message, 'content') else str(last_message)\n",
    "    \n",
    "    if \"Final Decision:\" in content:\n",
    "        decision_part = content.split(\"Final Decision:\")[1].strip()\n",
    "        if \"legitimate\" in decision_part.lower():\n",
    "            verdict = \"legitimate\"\n",
    "        elif \"suspicious\" in decision_part.lower():\n",
    "            verdict = \"suspicious\"\n",
    "        elif \"fraudulent\" in decision_part.lower():\n",
    "            verdict = \"fraudulent\"\n",
    "        else:\n",
    "            verdict = \"suspicious\"\n",
    "    else:\n",
    "        risk_score = state[\"context\"].get(\"risk_score\", 0.5)\n",
    "        if risk_score > 0.8:\n",
    "            verdict = \"fraudulent\"\n",
    "        elif risk_score > 0.6:\n",
    "            verdict = \"suspicious\"\n",
    "        else:\n",
    "            verdict = \"legitimate\"\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"final_verdict\": verdict,\n",
    "        \"current_step\": \"final_decision\",\n",
    "        \"decision\": \"final_decision\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add nodes to workflow\n",
    "workflow.add_node(\"initial_assessment\", initial_assessment)\n",
    "workflow.add_node(\"llm_decision\", llm_decision)\n",
    "workflow.add_node(\"execute_tools\", execute_tools)\n",
    "workflow.add_node(\"final_decision\", final_decision)\n",
    "\n",
    "# Define workflow edges\n",
    "workflow.set_entry_point(\"initial_assessment\")\n",
    "workflow.add_edge(\"initial_assessment\", \"llm_decision\")\n",
    "\n",
    "def route_after_llm(state: FraudDetectionState) -> str:\n",
    "    if state[\"decision\"] == \"final_decision\":\n",
    "        return \"final_decision\"\n",
    "    return \"execute_tools\"\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"llm_decision\",\n",
    "    route_after_llm,\n",
    "    {\n",
    "        \"execute_tools\": \"execute_tools\",\n",
    "        \"final_decision\": \"final_decision\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"execute_tools\", \"llm_decision\")\n",
    "workflow.add_edge(\"final_decision\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample transaction\n",
    "transaction = {\n",
    "    \"amount\": \"8500\",\n",
    "    \"location\": \"FOREIGN_COUNTRY\",\n",
    "    \"time\": \"04:30\",\n",
    "    \"user_id\": \"user123\",\n",
    "    \"merchant_id\": \"merchant456\",\n",
    "    \"ip_address\": \"192.168.1.1\",\n",
    "    \"document_id\": \"DOC789\",\n",
    "    \"merchant_category\": \"gambling\"\n",
    "}\n",
    "\n",
    "# Initialize state\n",
    "initial_state = {\n",
    "    \"transaction\": transaction,\n",
    "    \"context\": {},\n",
    "    \"messages\": [],\n",
    "    \"decision\": \"continue\",\n",
    "    \"final_verdict\": \"legitimate\",\n",
    "    \"current_step\": \"start\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the workflow\n",
    "result = app.invoke(initial_state)\n",
    "\n",
    "# Print results\n",
    "print(\"\\n=== FRAUD DETECTION RESULTS ===\")\n",
    "print(f\"Transaction: {json.dumps(transaction, indent=2)}\")\n",
    "print(\"\\nWorkflow Execution:\")\n",
    "for i, msg in enumerate(result[\"messages\"]):\n",
    "    role = msg.type\n",
    "    content = msg.content\n",
    "    print(f\"\\n--- Step {i+1} ({role.upper()}) ---\")\n",
    "    print(content)\n",
    "\n",
    "print(\"\\n=== CONTEXT SUMMARY ===\")\n",
    "for key, value in result[\"context\"].items():\n",
    "    if key != \"processed_data\":\n",
    "        print(f\"{key}: {json.dumps(value, indent=2)}\")\n",
    "\n",
    "print(f\"\\n=== FINAL VERDICT ===\")\n",
    "print(f\"Decision: {result['final_verdict'].upper()}\")\n",
    "print(\"=== END OF REPORT ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
