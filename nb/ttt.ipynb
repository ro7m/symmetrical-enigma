{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eah6-Vfr3Av9"
      },
      "source": [
        "# Fraud Detection Workflow with Transformers\n",
        "\n",
        "This notebook implements an agentic workflow for fraud detection using LangGraph and local Transformers models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXlgw63p3Av9"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q langgraph langchain-core langchain-community transformers accelerate bitsandbytes google-colab\n",
        "\n",
        "# Import libraries\n",
        "\n",
        "pip install litellm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qr_wpOdQbcl5"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, ToolMessage\n",
        "from langchain_community.chat_models import ChatLiteLLM\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.graph import StateGraph, END\n",
        "from typing import Annotated, Sequence\n",
        "import operator\n",
        "llm = ChatLiteLLM(model=\"ollama/mistral\",temperature=0.7)\n",
        "tools = [calculator]\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "# Define tools\n",
        "@tool\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "@tool\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Adds a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "@tool\n",
        "def divide(a: int, b: int) -> float:\n",
        "    \"\"\"Divide a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a / b\n",
        "\n",
        "\n",
        "# Augment the LLM with tools\n",
        "tools = [add, multiply, divide]\n",
        "tools_by_name = {tool.name: tool for tool in tools}\n",
        "llm_with_tools = llm.bind_tools(tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7IMNpysPbxGg"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import MessagesState\n",
        "from langchain_core.messages import ToolMessage, SystemMessage # Import SystemMessage\n",
        "from typing import Literal # Import Literal\n",
        "\n",
        "# Nodes\n",
        "def llm_call(state: MessagesState):\n",
        "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
        "\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            llm_with_tools.invoke(\n",
        "                [\n",
        "                    SystemMessage(\n",
        "                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n",
        "                    )\n",
        "                ]\n",
        "                + state[\"messages\"]\n",
        "            )\n",
        "        ]\n",
        "    }\n",
        "\n",
        "\n",
        "def tool_node(state: dict):\n",
        "    \"\"\"Performs the tool call\"\"\"\n",
        "\n",
        "    result = []\n",
        "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
        "        tool = tools_by_name[tool_call[\"name\"]]\n",
        "        observation = tool.invoke(tool_call[\"args\"])\n",
        "        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n",
        "    return {\"messages\": result}\n",
        "\n",
        "\n",
        "# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\n",
        "def should_continue(state: MessagesState) -> Literal[\"environment\", END]:\n",
        "    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\n",
        "\n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1]\n",
        "    # If the LLM makes a tool call, then perform an action\n",
        "    if last_message.tool_calls:\n",
        "        return \"Action\" # Should be \"tools\" as per graph node name\n",
        "    # Otherwise, we stop (reply to the user)\n",
        "    return END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBKEaoS1cGKI"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Build graph\n",
        "workflow = StateGraph(MessagesState)\n",
        "\n",
        "# Add nodes\n",
        "workflow.add_node(\"llm_call\", llm_call)\n",
        "workflow.add_node(\"tool_node\", tool_node)\n",
        "\n",
        "# Set entry point\n",
        "workflow.set_entry_point(\"llm_call\")\n",
        "\n",
        "# Add edges\n",
        "workflow.add_conditional_edges(\n",
        "    \"llm_call\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"Action\": \"tool_node\", # Route to tool_node if the LLM calls a tool\n",
        "        END: END # End if the LLM does not call a tool\n",
        "    }\n",
        ")\n",
        "workflow.add_edge(\"tool_node\", \"llm_call\") # After executing tools, call the LLM again\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile()"
      ]
    },
    {
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# Define the initial input message\n",
        "inputs = {\"messages\": [HumanMessage(content=\"What is 2 + 2?\")]}\n",
        "\n",
        "# Invoke the graph\n",
        "result = app.invoke(inputs)\n",
        "\n",
        "# Print the result\n",
        "print(result)"
      ]
    }
  ],
  "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
  "nbformat": 4,
  "nbformat_minor": 0
}
