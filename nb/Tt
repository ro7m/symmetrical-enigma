# --- Update the SimpleLLM.invoke method ---
class SimpleLLM:
    def __init__(self, pipeline):
        self.pipeline = pipeline

    def invoke(self, messages):
        # Extract the last message content safely for the prompt
        prompt_text = ""
        if isinstance(messages, list) and messages:
            last_message = messages[-1]
            # Handle different message object types
            if hasattr(last_message, 'content') and isinstance(last_message.content, str):
                prompt_text = last_message.content
            elif isinstance(last_message, str):
                 # Fallback if the last item is a raw string (less likely in this setup, but safe)
                 prompt_text = last_message
            else:
                # Fallback to string representation if content is not a simple string
                prompt_text = str(last_message)
        else:
            prompt_text = str(messages)

        # Generate text
        try:
            # Ensure prompt is a string
            prompt_text = str(prompt_text)
            outputs = self.pipeline(prompt_text)
            generated_text = outputs[0]['generated_text']
            # Remove the input prompt from the output if it's included (common with transformers)
            if generated_text.startswith(prompt_text):
                generated_text = generated_text[len(prompt_text):].strip()

            # --- Crucial Fix: Ensure the response object has the expected structure ---
            # The error might occur if the response object isn't handled correctly downstream.
            # Creating an AIMessage is the standard practice in LangGraph/LangChain workflows.
            response_message = AIMessage(content=generated_text)

        except Exception as e:
            error_msg = f"Error generating response: {str(e)}"
            # Return an AIMessage even on error for consistency
            response_message = AIMessage(content=error_msg)

        return response_message # Return an AIMessage object

# --- Update llm_decision node ---
def llm_decision(state: FraudDetectionState) -> FraudDetectionState:
    """LLM makes decision about next action or final verdict"""
    messages = state["messages"]

    # Get LLM response (should now be an AIMessage)
    response = llm.invoke(messages)

    # --- Safely check for final decision in the response content ---
    is_final = False
    # Ensure response has content and it's a string before checking
    response_content = getattr(response, 'content', '') # Safer way to access content
    if isinstance(response_content, str) and "Final Decision:" in response_content:
        is_final = True

    return {
        **state,
        "messages": messages + [response], # Append the AIMessage
        "decision": "final_decision" if is_final else "continue",
        "current_step": "llm_decision"
    }

# --- Update final_decision node ---
def final_decision(state: FraudDetectionState) -> FraudDetectionState:
    """Extract and record the final decision"""
    messages = state["messages"]
    last_message = messages[-1]

    # --- Safely get content from the last message (could be HumanMessage, AIMessage, ToolMessage) ---
    # Use getattr with a default empty string
    content = getattr(last_message, 'content', '')

    verdict = "suspicious" # Default verdict

    if isinstance(content, str) and "Final Decision:" in content:
        try:
            decision_part = content.split("Final Decision:")[1].strip()
            if "legitimate" in decision_part.lower():
                verdict = "legitimate"
            elif "suspicious" in decision_part.lower():
                verdict = "suspicious"
            elif "fraudulent" in decision_part.lower():
                verdict = "fraudulent"
            # else: keep default 'suspicious'
        except IndexError:
            # Handle potential issues with splitting
            verdict = "suspicious"
    else:
        # Fallback based on risk score if "Final Decision:" marker is missing
        risk_score = state["context"].get("risk_score", 0.5)
        if risk_score > 0.8:
            verdict = "fraudulent"
        elif risk_score > 0.6:
            verdict = "suspicious"
        else:
            verdict = "legitimate"

    return {
        **state,
        "final_verdict": verdict,
        "current_step": "final_decision",
        "decision": "final_decision"
    }

# --- No changes needed for initial_assessment or execute_tools as they correctly create HumanMessage objects ---
# --- The workflow compilation and execution code also remains the same ---
